{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import join as pj\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as tv_models\n",
    "import visdom\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # experiment_name\n",
    "    experiment_name = \"insect2vec_b20_epoch1000\"\n",
    "    # paths\n",
    "    all_data_path = \"/home/tanida/workspace/Insect_project/data/all_classification_data/classify_insect_std\"\n",
    "    model_save_path_root = pj(\"/home/tanida/workspace/Insect_project/model/classification/insect2vec\", experiment_name)\n",
    "    semantic_save_path_root = pj(\"/home/tanida/workspace/Insect_project/data/insect_semantic_vector\", experiment_name)\n",
    "    # train config\n",
    "    vector_length = 100\n",
    "    bs = 20\n",
    "    num_workers = 20\n",
    "    lr = 1e-5\n",
    "    alpha = 1e-2\n",
    "    lamda = 1e-1\n",
    "    save_interval = 100\n",
    "    nepoch = 1000\n",
    "    # visdom\n",
    "    visdom = True\n",
    "    port = 8098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, file_root, filename):\n",
    "        self.file_path = pj(file_root, filename)\n",
    "        if os.path.exists(file_root) is False:\n",
    "            os.makedirs(file_root)\n",
    "\n",
    "    def write(self, msg):\n",
    "        if self.file_path is not None:\n",
    "            with open(self.file_path, \"a\") as f:\n",
    "                f.write(msg)\n",
    "\n",
    "def generate_args_map(args):\n",
    "    import re\n",
    "    args_keys_list = list(args.__dict__.keys())\n",
    "    args_values_list = list(args.__dict__.values())\n",
    "\n",
    "    pattern = r\"__\"\n",
    "    refined_args_map = {}\n",
    "    for i, args_key in enumerate(args_keys_list):\n",
    "        is_meta = re.match(pattern, args_key)\n",
    "        if is_meta is None:\n",
    "            refined_args_map.update({args_keys_list[i]:args_values_list[i]})\n",
    "    return refined_args_map\n",
    "\n",
    "def save_experiment_args(args_logger, args):\n",
    "    args_logger.write(\"\\nTraining on: \" + args.experiment_name + \"\\n\")\n",
    "    args_logger.write(\"Using the specified args:\"+\"\\n\")\n",
    "    for k,v in args_map.items():\n",
    "        args_logger.write(str(k)+\": \"+str(v)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.visdom:\n",
    "    # create visdom\n",
    "    vis = visdom.Visdom(port=args.port)\n",
    "    \n",
    "    win_match_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='match_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_norm_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='norm_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_all_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='train_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(vis, phase, visualized_data, window):\n",
    "    vis.line(\n",
    "        X=np.array([phase]),\n",
    "        Y=np.array([visualized_data]),\n",
    "        update='append',\n",
    "        win=window\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class insect_dataset(data.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        images, labels, class_num, class_count = self.load_data(data_path)\n",
    "        self.images = torch.from_numpy(images).transpose(1, -1).float()\n",
    "        self.labels = torch.from_numpy(labels)\n",
    "        self.class_num = class_num\n",
    "        self.class_count = class_count\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def load_data(self, data_path):\n",
    "        with h5py.File(data_path) as f:\n",
    "            X = f[\"X\"][:]\n",
    "            Y = f[\"Y\"][:]\n",
    "        idx, count = np.unique(Y, return_counts=True)\n",
    "        return X, Y, len(idx), count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img2vec(nn.Module):\n",
    "    def __init__(self, img_size, training=True, vector_length=args.vector_length):\n",
    "        super(img2vec, self).__init__()\n",
    "        last_pool_size = img_size\n",
    "        for i in range(4):\n",
    "            last_pool_size = (int)(last_pool_size / 2)\n",
    "        self.training = training\n",
    "        self.vector_length = vector_length\n",
    "        # encoder\n",
    "        self.conv11 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        self.conv21 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv22 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        self.conv31 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv32 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv33 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        self.conv41 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv42 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv43 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "        self.conv51 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv52 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv53 = nn.Conv2d(512, vector_length, kernel_size=3, padding=1)\n",
    "        self.maxpool5 = nn.MaxPool2d(last_pool_size, return_indices=True)\n",
    "        \n",
    "        # decoder\n",
    "        self.maxunpool5 = nn.MaxUnpool2d(last_pool_size)\n",
    "        self.conv53d = nn.Conv2d(vector_length, 512, kernel_size=3, padding=1)\n",
    "        self.conv52d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv51d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.maxunpool4 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.conv43d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv42d = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv41d = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.maxunpool3 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.conv33d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv32d = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv31d = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.maxunpool2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.conv22d = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv21d = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.maxunpool1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.conv12d = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv11d = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        size1b = x.size()\n",
    "        x, indices1b = self.maxpool1(x)\n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = F.relu(self.conv22(x))\n",
    "        size2b = x.size()\n",
    "        x, indices2b = self.maxpool2(x)\n",
    "        x = F.relu(self.conv31(x))\n",
    "        x = F.relu(self.conv32(x))\n",
    "        x = F.relu(self.conv33(x))\n",
    "        size3b = x.size()\n",
    "        x, indices3b = self.maxpool3(x)\n",
    "        x = F.relu(self.conv41(x))\n",
    "        x = F.relu(self.conv42(x))\n",
    "        x = F.relu(self.conv43(x))\n",
    "        size4b = x.size()\n",
    "        x, indices4b = self.maxpool4(x)\n",
    "        x = F.relu(self.conv51(x))\n",
    "        x = F.relu(self.conv52(x))\n",
    "        x = F.relu(self.conv53(x))\n",
    "        size5b = x.size()\n",
    "        x, indices5b = self.maxpool5(x)\n",
    "        \n",
    "        # decoder\n",
    "        if self.training is True:\n",
    "            x = self.maxunpool5(x, indices5b, output_size=size5b)\n",
    "            x = F.relu(self.conv53d(x))\n",
    "            x = F.relu(self.conv52d(x))\n",
    "            x = F.relu(self.conv51d(x))\n",
    "            x = self.maxunpool4(x, indices4b, output_size=size4b)\n",
    "            x = F.relu(self.conv43d(x))\n",
    "            x = F.relu(self.conv42d(x))\n",
    "            x = F.relu(self.conv41d(x))\n",
    "            x = self.maxunpool3(x, indices3b, output_size=size3b)\n",
    "            x = F.relu(self.conv33d(x))\n",
    "            x = F.relu(self.conv32d(x))\n",
    "            x = F.relu(self.conv31d(x))\n",
    "            x = self.maxunpool2(x, indices2b, output_size=size2b)\n",
    "            x = F.relu(self.conv22d(x))\n",
    "            x = F.relu(self.conv21d(x))\n",
    "            x = self.maxunpool1(x, indices1b, output_size=size1b)\n",
    "            x = F.relu(self.conv12d(x))\n",
    "            x = F.relu(self.conv11d(x))\n",
    "        return x\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = nn.MSELoss(reduction='elementwise_mean').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_epoch(model, data_loader, optimizer, epoch, use_visdom, alpha=1e-3, lamda=1e-1):\n",
    "    model.train()\n",
    "    total_match_loss = 0\n",
    "    total_norm_loss = 0\n",
    "    \n",
    "    # train\n",
    "    for image, label in tqdm(data_loader, leave=False):\n",
    "        #print(image.shape)\n",
    "        image = image.cuda()\n",
    "        \n",
    "        # forward\n",
    "        output = model(image)\n",
    "        \n",
    "        # calculate loss\n",
    "        optimizer.zero_grad()\n",
    "        match_loss = l2_loss(output, image)\n",
    "        match_loss = alpha * match_loss\n",
    "        loss = match_loss\n",
    "        \n",
    "        if args.lamda != 0:\n",
    "            norm_loss = 0\n",
    "            for param in model.parameters():\n",
    "                param_target = torch.zeros(param.size()).cuda()\n",
    "                norm_loss += l2_loss(param, param_target)\n",
    "\n",
    "            norm_loss = lamda * norm_loss\n",
    "            loss += norm_loss\n",
    "        else:\n",
    "            norm_loss = 0\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_match_loss += match_loss.item()\n",
    "        total_norm_loss += norm_loss.item()\n",
    "    \n",
    "    print('epoch ' + str(epoch) + ' || MATCH Loss: %.4f NORM Loss: %.4f ||' % (total_match_loss, total_norm_loss))\n",
    "    \n",
    "    if use_visdom:\n",
    "        visualize(vis, epoch+1, total_match_loss, win_match_loss)\n",
    "        visualize(vis, epoch+1, total_norm_loss, win_norm_loss)\n",
    "        visualize(vis, epoch+1, total_match_loss + total_norm_loss, win_all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_map = generate_args_map(args)\n",
    "args_logger = Logger(args.model_save_path_root, \"args.txt\")\n",
    "save_experiment_args(args_logger, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = insect_dataset(args.all_data_path)\n",
    "train_data_loader = data.DataLoader(train_dataset, args.bs, num_workers=args.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = img2vec(200, training = True).cuda()\n",
    "model.apply(weights_init)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(args.nepoch):\n",
    "    train_per_epoch(model, train_data_loader, optimizer, epoch, args.visdom, alpha=args.alpha, lamda=args.lamda)\n",
    "    \n",
    "    # save model\n",
    "    if epoch != 0 and epoch % args.save_interval == 0:\n",
    "        print('Saving state, epoch: ' + str(epoch))\n",
    "        torch.save(model.state_dict(), pj(args.model_save_path_root, 'insect2vec_{}.pth'.format(str(epoch))))\n",
    "\n",
    "# save final model\n",
    "print('Saving state, final')\n",
    "torch.save(model.state_dict(), pj(args.model_save_path_root, 'insect2vec_final.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output semantic vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_semantic_vector(model, data_loader, class_num, class_count, vector_length):\n",
    "    model.eval()\n",
    "    semantic_vector = np.zeros((class_num, vector_length))\n",
    "\n",
    "    for image, label in tqdm(data_loader, leave=False):\n",
    "        image = image.cuda()\n",
    "        output_vector = model(image)\n",
    "        output_vector = torch.squeeze(output_vector)\n",
    "        output_vector = F.normalize(output_vector, p=2, dim=0)\n",
    "        output_vector = output_vector.cpu().detach().numpy()\n",
    "        semantic_vector[label] += output_vector\n",
    "    \n",
    "    for i,count in enumerate(class_count):\n",
    "        semantic_vector[i] = semantic_vector[i] / count\n",
    "    return semantic_vector\n",
    "\n",
    "def write_semantic_vector(semantic_vector, semantic_save_path_root, vector_length):\n",
    "    save_path = pj(semantic_save_path_root, \"vectors.txt\")\n",
    "    semantic_string = \"\"\n",
    "    for vector in semantic_vector:\n",
    "        for i,num in enumerate(vector):\n",
    "            if i == vector_length - 1:\n",
    "                semantic_string += str(num) + \"\\n\"\n",
    "            else:\n",
    "                semantic_string += str(num) + \" \"\n",
    "    if os.path.exists(semantic_save_path_root) is False:\n",
    "        os.makedirs(semantic_save_path_root)\n",
    "    with open(save_path, mode=\"w\") as f:\n",
    "        f.write(semantic_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = insect_dataset(args.all_data_path)\n",
    "class_num = test_dataset.class_num\n",
    "class_count = test_dataset.class_count\n",
    "test_data_loader = data.DataLoader(test_dataset, 1, num_workers=args.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = img2vec(200, training = False).cuda()\n",
    "model.load_state_dict(torch.load(pj(args.model_save_path_root, 'insect2vec_final.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_vector = output_semantic_vector(model, test_data_loader, class_num, class_count, args.vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_semantic_vector(semantic_vector, args.semantic_save_path_root, args.vector_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_040",
   "language": "python",
   "name": "pytorch_040"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
