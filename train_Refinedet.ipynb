{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import *\n",
    "from os.path import join as pj\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import visdom\n",
    "\n",
    "# Logger\n",
    "from IO.logger import Logger\n",
    "# Optimizer\n",
    "from model.optimizer import AdamW\n",
    "# Dataset\n",
    "from dataset.detection.dataset import insects_dataset_from_voc_style_txt, collate_fn\n",
    "# Loss Function\n",
    "from model.refinedet.loss.multiboxloss import RefineDetMultiBoxLoss\n",
    "# Model initializer\n",
    "from model.refinedet.refinedet import RefineDet\n",
    "# Predict\n",
    "from model.refinedet.utils.predict import test_prediction\n",
    "# Evaluate\n",
    "from evaluation.detection.evaluate import Voc_Evaluater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # experiment name\n",
    "    experiment_name = \"crop_b2_2_4_8_16_32_im512_xavier_uniform_\"\n",
    "    # paths\n",
    "    data_root = \"/home/tanida/workspace/Insect_Phenology_Detector/data\"\n",
    "    train_image_root = \"/home/tanida/workspace/Insect_Phenology_Detector/data/train_refined_images\"\n",
    "    train_target_root = \"/home/tanida/workspace/Insect_Phenology_Detector/data/train_detection_data/refinedet_all\"\n",
    "    test_image_root = \"/home/tanida/workspace/Insect_Phenology_Detector/data/test_refined_images\"\n",
    "    test_target_root = \"/home/tanida/workspace/Insect_Phenology_Detector/data/test_detection_data/refinedet_all\"\n",
    "    model_root = pj(\"/home/tanida/workspace/Insect_Phenology_Detector/output_model/detection/RefineDet\", experiment_name)\n",
    "    prc_root = pj(\"/home/tanida/workspace/Insect_Phenology_Detector/output_model/detection/RefineDet\", experiment_name)\n",
    "    # training config\n",
    "    input_size = 512 # choices=[320, 512, 1024]\n",
    "    crop_num = (5, 5)\n",
    "    batch_size = 2\n",
    "    num_workers = 2\n",
    "    lr = 1e-4\n",
    "    lamda = 1e-4\n",
    "    tcb_layer_num = 5\n",
    "    use_extra_layer = False\n",
    "    max_epoch = 100\n",
    "    valid_interval = 2\n",
    "    save_interval = 20\n",
    "    pretrain = True\n",
    "    freeze = False\n",
    "    optimizer = \"AdamW\"\n",
    "    activation_function = \"ReLU\"\n",
    "    init_function = \"xavier_uniform_\"\n",
    "    use_CSL = False\n",
    "    CSL_weight = [0.8, 1.2]\n",
    "    # visualization\n",
    "    visdom = True\n",
    "    visdom_port = 8097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.visdom:\n",
    "    # Create visdom\n",
    "    vis = visdom.Visdom(port=args.visdom_port)\n",
    "    \n",
    "    \"\"\"train_lossl\"\"\"\n",
    "    win_arm_loc = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='arm_loc_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_arm_conf = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='arm_conf_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_odm_loc = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='odm_loc_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_odm_conf = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='odm_conf_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_norm_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='normalization_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_all_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='train_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_train_acc = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='train_accuracy',\n",
    "            xlabel='epoch',\n",
    "            ylabel='average precision',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_test_acc = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='test_accuracy',\n",
    "            xlabel='epoch',\n",
    "            ylabel='average precision',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(phase, visualized_data, window):\n",
    "    vis.line(\n",
    "        X=np.array([phase]),\n",
    "        Y=np.array([visualized_data]),\n",
    "        update='append',\n",
    "        win=window\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_criterion = RefineDetMultiBoxLoss(2, use_ARM=False, use_CSL=False, CSL_weight=args.CSL_weight)\n",
    "odm_criterion = RefineDetMultiBoxLoss(2, use_ARM=True, use_CSL=False, CSL_weight=args.CSL_weight)\n",
    "l2_loss = nn.MSELoss(reduction='mean').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_epoch(model, data_loader, optimizer, epoch):\n",
    "    # set refinedet to train mode\n",
    "    model.train()\n",
    "\n",
    "    # create loss counters\n",
    "    arm_loc_loss = 0\n",
    "    arm_conf_loss = 0\n",
    "    odm_loc_loss = 0\n",
    "    odm_conf_loss = 0\n",
    "    all_norm_loss = 0\n",
    "\n",
    "    # train\n",
    "    for images, targets, _, _, _ in tqdm(data_loader, leave=False):\n",
    "        imgs = np.asarray(images[0])\n",
    "        tars = targets[0]\n",
    "\n",
    "        refined_imgs = []\n",
    "        refined_tars = []\n",
    "        # refine imgs, tars\n",
    "        for i in range(imgs.shape[0]):\n",
    "            if tars[i].size(0) > 0:\n",
    "                refined_imgs.append(imgs[i])\n",
    "                refined_tars.append(tars[i])\n",
    "        imgs = np.asarray(refined_imgs)\n",
    "        tars = refined_tars\n",
    "\n",
    "        # define batch_num\n",
    "        if (imgs.shape[0]%args.batch_size == 0):\n",
    "            batch_num = int(imgs.shape[0]/args.batch_size)\n",
    "        else:\n",
    "            batch_num = int(imgs.shape[0]/args.batch_size) + 1\n",
    "\n",
    "        # random sample of batch\n",
    "        iter_batch = choice(range(batch_num), batch_num, replace=False)\n",
    "\n",
    "        # train for cropped image\n",
    "        for i in iter_batch:\n",
    "            images = imgs[i*args.batch_size:(i+1)*args.batch_size]\n",
    "            targets = tars[i*args.batch_size:(i+1)*args.batch_size]\n",
    "\n",
    "            # set cuda\n",
    "            images = torch.from_numpy(images).cuda()\n",
    "            targets = [ann.cuda() for ann in targets]\n",
    "\n",
    "            # forward\n",
    "            out = model(images)\n",
    "\n",
    "            # calculate loss\n",
    "            optimizer.zero_grad()\n",
    "            arm_loss_l, arm_loss_c = arm_criterion(out, targets)\n",
    "            odm_loss_l, odm_loss_c = odm_criterion(out, targets)\n",
    "            arm_loss = arm_loss_l + arm_loss_c\n",
    "            odm_loss = odm_loss_l + odm_loss_c\n",
    "            loss = arm_loss + odm_loss\n",
    "\n",
    "            if args.lamda != 0:\n",
    "                norm_loss = 0\n",
    "                for param in model.parameters():\n",
    "                    param_target = torch.zeros(param.size()).cuda()\n",
    "                    norm_loss += l2_loss(param, param_target)\n",
    "\n",
    "                norm_loss = norm_loss * args.lamda\n",
    "                loss += norm_loss\n",
    "            else:\n",
    "                norm_loss = 0\n",
    "\n",
    "            if torch.isnan(loss) == 0:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                arm_loc_loss += arm_loss_l.item()\n",
    "                arm_conf_loss += arm_loss_c.item()\n",
    "                odm_loc_loss += odm_loss_l.item()\n",
    "                odm_conf_loss += odm_loss_c.item()\n",
    "                all_norm_loss += norm_loss.item()\n",
    "\n",
    "    print('epoch ' + str(epoch) + ' || ARM_L Loss: %.4f ARM_C Loss: %.4f ODM_L Loss: %.4f ODM_C Loss: %.4f NORM Loss: %.4f ||' \\\n",
    "    % (arm_loc_loss, arm_conf_loss, odm_loc_loss, odm_conf_loss, all_norm_loss))\n",
    "\n",
    "    # visualize\n",
    "    if args.visdom:\n",
    "        visualize(epoch+1, arm_loc_loss, win_arm_loc)\n",
    "        visualize(epoch+1, arm_conf_loss, win_arm_conf)\n",
    "        visualize(epoch+1, odm_loc_loss, win_odm_loc)\n",
    "        visualize(epoch+1, odm_conf_loss, win_odm_conf)\n",
    "        visualize(epoch+1, all_norm_loss, win_norm_loss)\n",
    "        visualize(epoch+1, arm_loc_loss + arm_conf_loss + odm_loc_loss + odm_conf_loss + all_norm_loss, win_all_loss)\n",
    "\n",
    "def validate(evaluater, model, data_loader, crop_num, num_classes=2, nms_thresh=0.3):\n",
    "    result = test_prediction(model, data_loader, crop_num, num_classes, nms_thresh)\n",
    "    evaluater.set_result(result)\n",
    "    eval_metrics = evaluater.get_eval_metrics()\n",
    "    return eval_metrics[0]['AP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_logger = Logger(args)\n",
    "args_logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading dataset for train ...')\n",
    "train_dataset = insects_dataset_from_voc_style_txt(args.train_image_root, args.input_size, args.crop_num, \"RefineDet\", training=True, target_root=args.train_target_root)\n",
    "train_data_loader = data.DataLoader(train_dataset, 1, num_workers=1, shuffle=True, collate_fn=collate_fn)\n",
    "print('Loading dataset for test ...')\n",
    "test_dataset = insects_dataset_from_voc_style_txt(args.test_image_root, args.input_size, args.crop_num, \"RefineDet\", training=False)\n",
    "test_data_loader = data.DataLoader(test_dataset, 1, num_workers=1, shuffle=False, collate_fn=collate_fn)\n",
    "train_valid_dataset = insects_dataset_from_voc_style_txt(args.train_image_root, args.input_size, args.crop_num, \"RefineDet\", training=False)\n",
    "train_valid_data_loader = data.DataLoader(train_valid_dataset, 1, num_workers=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RefineDet(args.input_size, 2, args.tcb_layer_num, pretrain=args.pretrain, freeze=args.freeze, activation_function=args.activation_function, init_function=args.init_function, use_extra_layer=args.use_extra_layer)\n",
    "if args.optimizer == \"AdamW\":\n",
    "    print(\"optimizer = AdamW\")\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "else:\n",
    "    print(\"optimizer = Adam\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(pj(args.prc_root, \"train\")) is False:\n",
    "    os.makedirs(pj(args.prc_root, \"train\"))\n",
    "if os.path.exists(pj(args.prc_root, \"test\")) is False:\n",
    "    os.makedirs(pj(args.prc_root, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_evaluater = Voc_Evaluater(args.train_image_root, args.train_target_root, pj(args.prc_root, \"train\"))\n",
    "test_evaluater = Voc_Evaluater(args.test_image_root, args.test_target_root, pj(args.prc_root, \"test\"))\n",
    "for epoch in range(args.max_epoch):\n",
    "    train_per_epoch(model, train_data_loader, optimizer, epoch)\n",
    "    \n",
    "    # validate model\n",
    "    if epoch != 0 and epoch % args.valid_interval == 0:\n",
    "        train_ap = validate(train_evaluater, model, train_valid_data_loader, args.crop_num, num_classes=2, nms_thresh=0.5)\n",
    "        test_ap = validate(test_evaluater, model, test_data_loader, args.crop_num, num_classes=2, nms_thresh=0.5)\n",
    "        print(\"epoch: {}, train_ap={}, test_ap={}\".format(epoch, train_ap, test_ap))\n",
    "        if args.visdom:\n",
    "            visualize(epoch+1, train_ap, win_train_acc)\n",
    "            visualize(epoch+1, test_ap, win_test_acc)\n",
    "    \n",
    "    # save model\n",
    "    if epoch != 0 and epoch % args.save_interval == 0:\n",
    "        print('Saving state, epoch: ' + str(epoch))\n",
    "        torch.save(model.state_dict(), args.model_root + '/RefineDet{}_{}.pth'.format(args.input_size, str(epoch)))\n",
    "\n",
    "# final save model\n",
    "print('Saving state, final')\n",
    "torch.save(model.state_dict(), args.model_root + '/RefineDet{}_final.pth'.format(args.input_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_100",
   "language": "python",
   "name": "pytorch_100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
