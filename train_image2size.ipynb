{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import h5py\n",
    "import imgaug.augmenters as iaa\n",
    "import numpy as np\n",
    "import os\n",
    "from os import getcwd as cwd\n",
    "from os.path import join as pj\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import visdom\n",
    "\n",
    "# Logger\n",
    "from IO.logger import Logger\n",
    "# Data Sampling\n",
    "from dataset.classification.sampler import adopt_sampling\n",
    "# model\n",
    "from model.optimizer import AdamW\n",
    "from model.resnet.resnet import ResNet\n",
    "from model.resnet.resnet_base import BasicBlock, Bottleneck, _resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    experiment_name = \"ResNet34_b80_lr1e-4_all02withResize\"\n",
    "    # paths\n",
    "    all_data_path = pj(cwd(), \"data/all_classification_data\", \"classify_insect_std_20200806_with_size\")\n",
    "    figure_root = pj(cwd(), \"figure/image2size\", experiment_name)\n",
    "    model_root = pj(cwd(), \"output_model/image2size\", experiment_name)\n",
    "    # train config\n",
    "    model_name = \"resnet34\" # choice in [\"resnet18\", \"resnet34\"]\n",
    "    bs = 80\n",
    "    lr = 1e-4\n",
    "    nepoch = 300\n",
    "    weight_func = None  # choice [None, \"Pow\", \"LowSensitivePow\"]\n",
    "    weight_power = 2  # adopt exponent to train loss\n",
    "    pretrain = True\n",
    "    param_freeze = False\n",
    "    sampling = \"OverSample\" # choice [None, \"RandomSample\", \"OverSample\"]\n",
    "    activation_function = \"ReLU\" # choice [\"ReLU\", \"LeakyReLU\", \"RReLU\"]\n",
    "    decoder = None # choice [None, \"Concatenate\", \"FPN\"]\n",
    "    use_dropout = True\n",
    "    label_smooth = False\n",
    "    use_aug = True\n",
    "    use_aug_scale = False\n",
    "    # visdom\n",
    "    visdom = True\n",
    "    port = 8097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(args.figure_root) is False:\n",
    "    os.makedirs(args.figure_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    args.cuda = False\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_logger = Logger(args)\n",
    "args_logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.visdom:\n",
    "    # Create visdom\n",
    "    vis = visdom.Visdom(port=args.port)\n",
    "    \n",
    "    win_train_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='train_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_test_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='test_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(vis, phase, visualized_data, window):\n",
    "    vis.line(\n",
    "        X=np.array([phase]),\n",
    "        Y=np.array([visualized_data]),\n",
    "        update='append',\n",
    "        win=window\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image2size_dataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, images, sizes=None, label_smooth=False, use_aug=False, use_aug_scale=False, mode=\"test\"):\n",
    "        # choice mode in [\"train\", \"eval\", \"test\"]\n",
    "        self.images = images\n",
    "        self.sizes = sizes\n",
    "        self.mode = mode\n",
    "        self.label_smooth = label_smooth\n",
    "        self.use_aug = use_aug\n",
    "        self.use_aug_scale = use_aug_scale\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            print(\"label_smooth = {}\".format(label_smooth))\n",
    "            print(\"use_aug = {}\".format(use_aug))\n",
    "            print(\"use_aug_scale = {}\".format(use_aug_scale))\n",
    "        \n",
    "        #\"\"\"\n",
    "        aug_list = [\n",
    "            iaa.OneOf([\n",
    "                iaa.ShearX((-20, 20)),\n",
    "                iaa.ShearY((-20, 20))\n",
    "            ]),\n",
    "            iaa.OneOf([\n",
    "                iaa.TranslateX(px=(-20, 20)),\n",
    "                iaa.TranslateY(px=(-20, 20))\n",
    "            ]),\n",
    "            iaa.Rotate((-90, 90)),\n",
    "            iaa.pillike.Autocontrast(),\n",
    "            iaa.Invert(0.5),\n",
    "            iaa.pillike.Equalize(),\n",
    "            iaa.Solarize(0.5, threshold=(32, 128)),\n",
    "            iaa.color.Posterize(),\n",
    "            iaa.pillike.EnhanceContrast(),\n",
    "            iaa.pillike.EnhanceColor(),\n",
    "            iaa.pillike.EnhanceBrightness(),\n",
    "            iaa.pillike.EnhanceSharpness(),\n",
    "        ]\n",
    "        #\"\"\"\n",
    "        \"\"\"\n",
    "        aug_list = [\n",
    "            iaa.OneOf([\n",
    "                iaa.ShearX((-20, 20)),\n",
    "                iaa.ShearY((-20, 20))\n",
    "            ]),\n",
    "            iaa.OneOf([\n",
    "                iaa.TranslateX(px=(-20, 20)),\n",
    "                iaa.TranslateY(px=(-20, 20))\n",
    "            ]),\n",
    "            iaa.Rotate((-90, 90)),\n",
    "        ]\n",
    "        \"\"\"\n",
    "        self.aug_seq = iaa.SomeOf((0, 2), aug_list, random_order=True)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index].astype(\"uint8\")\n",
    "        if self.mode == \"train\":\n",
    "            size = self.sizes[index].astype(\"float32\")\n",
    "            if self.label_smooth is True:\n",
    "                size += (np.random.rand() - 0.5) * 5\n",
    "            if self.use_aug is True:\n",
    "                image = self.aug_seq(image=image)\n",
    "                if np.random.rand() < (2/13):\n",
    "                    ratio = np.random.rand() + 1.0  # 1.0 <= ratio < 2.0\n",
    "                    image, size = self.aug_scale(image, size, ratio=ratio)\n",
    "            if self.use_aug_scale is True:\n",
    "                ratio = np.random.rand() + 1.0  # 1.0 <= ratio < 2.0\n",
    "                image, size = self.aug_scale(image, size, ratio=ratio)\n",
    "        \n",
    "        image = image.astype(\"float32\")\n",
    "        image = cv2.normalize(image, image, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "        image = image.transpose(2,0,1).astype(\"float32\")\n",
    "        image = torch.from_numpy(image)\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            return image, size\n",
    "        elif self.mode == \"eval\":\n",
    "            size = self.sizes[index].astype(\"float32\")\n",
    "            return image, size\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def aug_scale(self, image, size, ratio=1.0):\n",
    "        image = image.astype(\"float32\")\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        src = np.array([[0.0, 0.0],[0.0, 1.0],[1.0, 0.0]], np.float32)\n",
    "        dest = src * ratio\n",
    "        h_diff = (ratio - 1) * (h / 2)\n",
    "        w_diff = (ratio - 1) * (w / 2)\n",
    "        dest[:, 0] -= w_diff\n",
    "        dest[:, 1] -= h_diff\n",
    "        affine = cv2.getAffineTransform(src, dest)\n",
    "        \n",
    "        image = cv2.warpAffine(image, affine, (200, 200), cv2.INTER_LANCZOS4)\n",
    "        size = size * ratio\n",
    "        return image.astype(\"uint8\"), size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_sensitive_pow(loss, power):\n",
    "    loss_pow = torch.pow(loss, power)\n",
    "    loss_filter = loss < 1.0\n",
    "    loss_pow[loss_filter] = loss[loss_filter]\n",
    "    return loss_pow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, test_dataloader, lr=1e-4, nepoch=100, visdom=False):\n",
    "    # define loss\n",
    "    train_l1_loss = nn.L1Loss(reduction='mean')\n",
    "    test_l1_loss = nn.L1Loss(reduction='mean')\n",
    "    print(\"weight_func == {}\".format(args.weight_func))\n",
    "    if args.weight_func == \"LowSensitivePow\":\n",
    "        weight_func = low_sensitive_pow\n",
    "    else:\n",
    "        weight_func = torch.pow\n",
    "    \n",
    "    # define optimizer\n",
    "    opt = AdamW(model.parameters(), lr=lr)\n",
    "    # set model train mode\n",
    "    model.train()\n",
    "    \n",
    "    # set best loss\n",
    "    best_total_test_avg_loss = 1e6\n",
    "    \n",
    "    for epoch in range(nepoch):\n",
    "        total_train_loss = 0\n",
    "        total_test_loss = 0\n",
    "        # train\n",
    "        count = 0\n",
    "        for image, size in train_dataloader:\n",
    "            count += 1\n",
    "            if args.cuda is True:\n",
    "                image = image.cuda()\n",
    "                size = size.cuda()\n",
    "            opt.zero_grad()\n",
    "            out = model(image)\n",
    "            train_loss = train_l1_loss(out, size[:, None])\n",
    "            if args.weight_func == \"Pow\" or args.weight_func == \"LowSensitivePow\":\n",
    "                train_loss = weight_func(train_loss, args.weight_power)\n",
    "            total_train_loss += train_loss.item()\n",
    "            train_loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        print(\"train: target_dist = {}, output_dist = {}\".format(size[0], out[0].item()))\n",
    "        total_train_avg_loss = total_train_loss / count\n",
    "        \n",
    "        # valid\n",
    "        model.eval()\n",
    "        count = 0\n",
    "        for image, size in test_dataloader:\n",
    "            count += 1\n",
    "            if args.cuda is True:\n",
    "                image = image.cuda()\n",
    "                size = size.cuda()\n",
    "            out = model(image)\n",
    "            test_loss = test_l1_loss(out, size)\n",
    "            total_test_loss += test_loss.item()\n",
    "            \n",
    "        print(\"test: target_dist = {}, output_dist = {}\".format(size[0], out[0].item()))\n",
    "        total_test_avg_loss = total_test_loss / count\n",
    "        model.train()\n",
    "        \n",
    "        if total_test_avg_loss < best_total_test_avg_loss:\n",
    "            best_total_test_avg_loss = total_test_avg_loss\n",
    "            torch.save(model.state_dict(), pj(args.model_root, \"valid_\" + str(valid_count) + \"_best.pth\"))\n",
    "            with open(pj(args.model_root, \"valid_\" + str(valid_count) + \"_best_loss.txt\"), mode=\"w\") as f:\n",
    "                f.write(\"epoch = {}, test_loss = {}\".format(epoch, total_test_avg_loss))\n",
    "        \n",
    "        if visdom:\n",
    "            visualize(vis, epoch+1, total_train_avg_loss, win_train_loss)\n",
    "            visualize(vis, epoch+1, total_test_avg_loss, win_test_loss)\n",
    "        print(\"epoch=%s: train_loss=%f, test_loss=%f\" % (epoch, total_train_avg_loss, total_test_avg_loss))\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_size(model, dataloader):\n",
    "    size_array = []\n",
    "    \n",
    "    model.eval()\n",
    "    for image, _ in dataloader:\n",
    "        if args.cuda is True:\n",
    "            image = image.cuda()\n",
    "        out = model(image)\n",
    "        size_array.extend(out.cpu().detach().numpy())\n",
    "\n",
    "    model.train()\n",
    "    return np.array(size_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with h5py.File(args.all_data_path) as f:\n",
    "    images = f[\"X\"][:]\n",
    "    labels = f[\"Y\"][:]\n",
    "    sizes = f[\"size\"][:]\n",
    "all_dataset = image2size_dataset(images, sizes, mode=\"eval\")\n",
    "all_dataloader = data.DataLoader(all_dataset, 1, num_workers=0, shuffle=False)\n",
    "    \n",
    "# define kfold\n",
    "kf = KFold(n_splits=5)\n",
    "valid_count = 0\n",
    "\n",
    "# cross validation\n",
    "total_eval_train_loss = 0\n",
    "total_eval_test_loss = 0\n",
    "total_eval_all_loss = 0\n",
    "for train_index, test_index in kf.split(images):\n",
    "    print(\"\")\n",
    "    valid_count += 1\n",
    "    print(\"----- valid {} -----\".format(valid_count))\n",
    "    print(\"\")\n",
    "    # create validation data\n",
    "    train_index = adopt_sampling(labels, train_index, args.sampling)\n",
    "    image_train, image_test = images[train_index], images[test_index]\n",
    "    size_train, size_test = sizes[train_index], sizes[test_index]\n",
    "    # create dataloader\n",
    "    train_dataset = image2size_dataset(image_train, size_train, label_smooth=args.label_smooth, use_aug=args.use_aug, use_aug_scale=args.use_aug_scale, mode=\"train\")\n",
    "    train_dataloader = data.DataLoader(train_dataset, args.bs, num_workers=0, shuffle=True)\n",
    "    valid_dataset = image2size_dataset(image_train, size_train, mode=\"eval\")\n",
    "    valid_dataloader = data.DataLoader(valid_dataset, 1, num_workers=0, shuffle=False)\n",
    "    test_dataset = image2size_dataset(image_test, size_test, mode=\"eval\")\n",
    "    test_dataloader = data.DataLoader(test_dataset, 1, num_workers=0, shuffle=False)\n",
    "    \n",
    "    # create model\n",
    "    model = ResNet(args.model_name, 1, pretrain=args.pretrain, param_freeze=args.param_freeze, use_dropout=args.use_dropout, activation_function=args.activation_function, decoder=args.decoder).cuda()\n",
    "    \n",
    "    # training\n",
    "    train(model, train_dataloader, test_dataloader, lr=args.lr, nepoch=args.nepoch, visdom=args.visdom)\n",
    "    \n",
    "    # evaluation\n",
    "    model.load_state_dict(torch.load(pj(args.model_root, \"valid_\" + str(valid_count) + \"_best.pth\")))\n",
    "    \n",
    "    estimated_size_array = estimate_size(model, valid_dataloader)\n",
    "    eval_train_loss = np.sum(np.abs(estimated_size_array - size_train)) / len(size_train)\n",
    "    total_eval_train_loss += eval_train_loss\n",
    "    \n",
    "    estimated_size_array = estimate_size(model, test_dataloader)\n",
    "    eval_test_loss = np.sum(np.abs(estimated_size_array - size_test)) / len(size_test)\n",
    "    total_eval_test_loss += eval_test_loss\n",
    "    \n",
    "    estimated_size_array = estimate_size(model, all_dataloader)\n",
    "    eval_all_loss = np.sum(np.abs(estimated_size_array - sizes)) / len(sizes)\n",
    "    total_eval_all_loss += eval_all_loss\n",
    "    \n",
    "    valid_loss = pd.DataFrame({\"train\": [eval_train_loss], \"test\": [eval_test_loss], \"all\": [eval_all_loss]})\n",
    "    valid_loss.to_csv(pj(args.figure_root, \"valid_loss_\" + str(valid_count) + \".csv\"))\n",
    "    \n",
    "    bbox_df_with_estimate_size = pd.DataFrame({\"size\": sizes})\n",
    "    bbox_df_with_estimate_size[\"eval_size\"] = estimated_size_array\n",
    "    bbox_df_with_estimate_size.to_csv(pj(args.figure_root, \"output_size_\" + str(valid_count) + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss = pd.DataFrame({\"train\": [total_eval_train_loss / 5], \"test\": [total_eval_test_loss / 5], \"all\": [total_eval_all_loss / 5]})\n",
    "valid_loss.to_csv(pj(args.figure_root, \"final_loss_\" + str(valid_count) + \".csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_100",
   "language": "python",
   "name": "pytorch_100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
