{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックの役割\n",
    "- 分類モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- ライブラリ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from os.path import join as pj\n",
    "from os import getcwd as cwd\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "import visdom\n",
    "\n",
    "# IO関連\n",
    "from IO.logger import Logger\n",
    "from IO.visdom import visualize\n",
    "# 分類データセットのロード\n",
    "from dataset.classification.loader import create_validation_split, load_validation_data\n",
    "# 分類データセットのサンプリング\n",
    "from dataset.classification.sampler import adopt_sampling\n",
    "# 分類データセットを構築\n",
    "from dataset.classification.dataset import insects_dataset\n",
    "# モデル\n",
    "from model.resnet.resnet import ResNet\n",
    "from model.resnet.predict import test_classification\n",
    "from model.resnet.loss import LabelSmoothingLoss\n",
    "from model.mobilenet.mobilenet import MobileNet\n",
    "from model.optimizer import AdamW, RAdam\n",
    "# 評価関数\n",
    "from evaluation.classification.evaluate import accuracy, confusion_matrix\n",
    "# 統計関数\n",
    "from evaluation.classification.statistics import compute_each_size_df, compute_all_size_df\n",
    "# 可視化関数\n",
    "from evaluation.classification.visualize import create_confusion_matrix, plot_df_distrib_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 学習コンフィグ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # 実験名\n",
    "    experiment_name: str = \"sample\"\n",
    "    # テストデータの割合\n",
    "    test_ratio: float = 0.2\n",
    "    # パス\n",
    "    all_data_path: str = pj(cwd(), \"data/all_classification_data/classify_insect_std_20200806\")\n",
    "    model_root: str = pj(cwd(), \"output_model/classification/resnet50/b20_lr1e-5\", experiment_name)\n",
    "    figure_root: str = pj(cwd(), \"figure/classification/resnet50/b20_lr1e-5\", experiment_name)\n",
    "    # 学習時の設定\n",
    "    model_name: str = \"resnet50\" # [\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\"]の中から一つ\n",
    "    bs: int = 20\n",
    "    lr: float = 1e-5\n",
    "    lamda: float = 0\n",
    "    nepoch: int = 100\n",
    "    pretrain: bool = True\n",
    "    param_freeze: bool = False\n",
    "    sampling: str = \"OverSample\" # [None, \"RandomSample\", \"OverSample\"]の中から一つ\n",
    "    method_aug: str = [\"All\"] # dataset.classification.dataset.create_aug_seqにあるものから選択(複数可)\n",
    "    optimizer: str = \"AdamW\" # [\"Adam, AdamW\", \"RAdam\"]の中から一つ\n",
    "    activation_function: str = \"ReLU\" # [\"ReLU\", \"LeakyReLU\", \"RReLU\"]の中から一つ\n",
    "    decoder: str = \"Concatenate\" # [None, \"Concatenate\", \"FPN\"]の中から一つ\n",
    "    label_smoothing: str = \"manual\" # [None, \"manual\", \"knowledge_distillation\"]の中から一つ\n",
    "    size_normalization: str = \"uniform\" # [None, \"mu\", \"sigma\", \"mu_sigma\", \"uniform\"]の中から一つ\n",
    "    use_dropout: bool = True\n",
    "    # テスト時の設定\n",
    "    save_fig: bool = True\n",
    "    save_df: bool = True\n",
    "    # Visdom\n",
    "    visdom: bool = False\n",
    "    port: int = 8097 # defaultは8097"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = args.all_data_path.split('/')[-1]\n",
    "if dataset_name == 'classify_insect_std':\n",
    "    args.labels = ['Diptera', 'Ephemeridae', 'Ephemeroptera', \n",
    "                   'Lepidoptera', 'Plecoptera', 'Trichoptera']\n",
    "elif dataset_name == 'classify_insect_std_resizeFAR':\n",
    "    args.labels = ['Diptera', 'Ephemeridae', 'Ephemeroptera', \n",
    "                   'Lepidoptera', 'Plecoptera', 'Trichoptera']\n",
    "elif dataset_name == 'classify_insect_std_resize':\n",
    "    args.labels = ['Diptera', 'Ephemeridae', 'Ephemeroptera', \n",
    "                   'Lepidoptera', 'Plecoptera', 'Trichoptera']\n",
    "elif dataset_name == 'classify_insect_std_plus_other':\n",
    "    args.labels = ['Diptera', 'Ephemeridae', 'Ephemeroptera', \n",
    "                   'Lepidoptera', 'Plecoptera', 'Trichoptera', 'Other']\n",
    "elif dataset_name == 'classify_insect_std_20200806':\n",
    "    args.labels = ['Diptera', 'Ephemeridae', 'Ephemeroptera', \n",
    "                   'Lepidoptera', 'Plecoptera', 'Trichoptera']\n",
    "elif dataset_name == 'classify_insect_std_20200806_DBSCAN':\n",
    "    args.labels = ['Diptera', 'Ephemeridae', 'Ephemeroptera', \n",
    "                   'Lepidoptera', 'Plecoptera', 'Trichoptera']\n",
    "elif dataset_name == 'classify_insect_20200806':\n",
    "    args.labels = ['Diptera', 'Ephemeridae', 'Ephemeroptera', \n",
    "                   'Lepidoptera', 'Plecoptera', 'Trichoptera']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 学習関連 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_knowledge(knowledge_path):\n",
    "    \"\"\"\n",
    "        knowledge(学習済みモデルの出力)を読み込む\n",
    "        引数:\n",
    "            - knowledge_path: str, knowledgeファイルのパス\n",
    "            knowledgeは全データに対して予測値を出し, 予測値をクラスごとに分けて平均を取ったもの\n",
    "    \"\"\"\n",
    "    knowledge = []\n",
    "    with open(knowledge_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(\"\\n\")[0]\n",
    "            line = line.split(\" \")\n",
    "            line = [float(element) for element in line]\n",
    "            knowledge.append(line)\n",
    "    \n",
    "    knowledge = np.array(knowledge)\n",
    "    return knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_loss(label_smoothing=None):\n",
    "    \"\"\"\n",
    "        誤差関数を定義\n",
    "        引数:\n",
    "            - label_smoothing: str, ラベル平滑化の方法\n",
    "            [None, \"manual\", \"knowledge_distillation\"]の中から一つ\n",
    "            manualは存在確率としてターゲットクラスに0.9, その他のクラスに0.1を等分割したものを与える\n",
    "            knowledge_distillationは別の学習済みモデルの出力を, 平滑化ラベルとして与える\n",
    "    \"\"\"\n",
    "    if label_smoothing == \"manual\":\n",
    "        print(\"cross_entropy_loss == LabelSmoothingLoss (manual)\")\n",
    "        ce = LabelSmoothingLoss(0.1, len(args.labels))\n",
    "    elif label_smoothing == \"knowledge_distillation\":\n",
    "        print(\"cross_entropy_loss == LabelSmoothingLoss (knowledge_distillation)\")\n",
    "        knowledge = read_knowledge(pj(cwd(), \"data/insect_knowledge/resnet50_b20_r45_lr1e-5_crossvalid_20200806_All/knowledge.txt\"))\n",
    "        ce = LabelSmoothingLoss(0.1, len(args.labels), knowledge=knowledge)\n",
    "    else:\n",
    "        print(\"cross_entropy_loss == nn.CrossEntropyLoss\")\n",
    "        ce = torch.nn.CrossEntropyLoss().cuda()\n",
    "    l2_loss = nn.MSELoss(reduction='mean').cuda()\n",
    "    return ce, l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_optimizer(lr, optimizer=\"AdamW\"):\n",
    "    \"\"\"\n",
    "        最適化器を定義\n",
    "        引数:\n",
    "            - lr: float, 学習率\n",
    "            - optimizer: str, 最適化器の種類\n",
    "            [\"Adam, AdamW\", \"RAdam\"]の中から一つ\n",
    "    \"\"\"\n",
    "    if optimizer == \"Adam\":\n",
    "        print(\"optimizer == Adam\")\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == \"AdamW\":\n",
    "        print(\"optimizer == AdamW\")\n",
    "        opt = AdamW(model.parameters(), lr=lr)\n",
    "    elif optimizer == \"RAdam\":\n",
    "        print(\"optimizer == RAdam\")\n",
    "        opt = RAdam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        print(\"error! optimizer is not defined\")\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_epoch(train_dataloader, opt, model, ce, l2_loss, lamda=0.):\n",
    "    \"\"\"\n",
    "        1epochの学習コード\n",
    "        引数:\n",
    "            - train_dataloader: データローダ\n",
    "            - opt: 最適化器\n",
    "            - model: モデル\n",
    "            - ce: クロスエントロピー誤差\n",
    "            - l2_loss: L2誤差\n",
    "            - lamda: float, モデル重み正規化での重み\n",
    "    \"\"\"\n",
    "    # set model train mode\n",
    "    model.train()\n",
    "    \n",
    "    for x, y in train_dataloader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        if len(out.shape) == 1:\n",
    "            # bsが1だとエラーとなるので, 空の次元を挿入\n",
    "            out = out[None, :]\n",
    "\n",
    "        cls_loss = ce(out, y)\n",
    "        if lamda != 0:\n",
    "            # モデル重みのL2正規化\n",
    "            norm_loss = 0\n",
    "            for param in model.parameters():\n",
    "                param_target = torch.zeros(param.size()).cuda()\n",
    "                norm_loss += l2_loss(param, param_target)\n",
    "\n",
    "            norm_loss = norm_loss * lamda\n",
    "            cls_loss_item = cls_loss.item()\n",
    "            norm_loss_item = norm_loss.item()\n",
    "            loss = cls_loss + norm_loss\n",
    "        else:\n",
    "            norm_loss = 0\n",
    "            cls_loss_item = cls_loss.item()\n",
    "            norm_loss_item = 0\n",
    "            loss = cls_loss\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_item = loss.item()\n",
    "    \n",
    "    return cls_loss_item, norm_loss_item, loss_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, test_dataloader, model, lr=1e-5, lamda=0, \\\n",
    "          nepoch=100, visdom=False, label_smoothing=None, optimizer=\"Adam\"):\n",
    "    \"\"\"\n",
    "        学習コード\n",
    "        引数:\n",
    "            - train_dataloader: 学習データローダ\n",
    "            - valid_dataloader: 学習評価データローダ\n",
    "            - test_dataloader: テストデータローダ\n",
    "            - model: モデル\n",
    "            - lr: float, 学習率\n",
    "            - lamda: float, モデル重み正規化での重み\n",
    "            - nepoch: int, 何エポック学習するか\n",
    "            - visdom: bool, visdomで可視化するかどうか\n",
    "            - label_smoothing: str, ラベル平滑化の方法\n",
    "            - optimizer: str, 最適化器の種類\n",
    "    \"\"\"\n",
    "    # define loss\n",
    "    ce, l2_loss = define_loss(label_smoothing=label_smoothing)\n",
    "    \n",
    "    # define optimizer\n",
    "    opt = define_optimizer(lr, optimizer=optimizer)\n",
    "    \n",
    "    # set best acc\n",
    "    best_te_acc = 0\n",
    "    \n",
    "    # training\n",
    "    for epoch in tqdm(range(nepoch),leave=False):\n",
    "        sum_cls_loss = 0\n",
    "        sum_norm_loss = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        # 1epochの学習\n",
    "        epoch_cls_loss, epoch_norm_loss, epoch_loss = train_per_epoch(train_dataloader, opt, model, ce, l2_loss, lamda=lamda)\n",
    "        sum_cls_loss += epoch_cls_loss\n",
    "        sum_norm_loss += epoch_norm_loss\n",
    "        total_loss += epoch_loss\n",
    "        \n",
    "        # 評価 (evalモードとtrainモードの切り替えを忘れずに)\n",
    "        model.eval()\n",
    "        valid_acc = accuracy(valid_dataloader, model)\n",
    "        te_acc = accuracy(test_dataloader, model)\n",
    "        model.train()\n",
    "        \n",
    "        # 最良性能ならモデルを保存\n",
    "        if te_acc > best_te_acc:\n",
    "            best_te_acc = te_acc\n",
    "            torch.save(model.state_dict(), pj(args.model_root, \"valid_\" + str(valid_count + 1) + \"_best.pth\"))\n",
    "            with open(pj(args.model_root, \"valid_\" + str(valid_count + 1) + \"_best_accuracy.txt\"), mode=\"w\") as f:\n",
    "                f.write(\"epoch = {}, test_acc = {}\".format(epoch, te_acc))\n",
    "        \n",
    "        # visdomで評価指標を可視化\n",
    "        if visdom:\n",
    "            visualize(vis, epoch+1, sum_cls_loss, win_cls_loss)\n",
    "            visualize(vis, epoch+1, sum_norm_loss, win_norm_loss)\n",
    "            visualize(vis, epoch+1, total_loss, win_train_loss)\n",
    "            visualize(vis, epoch+1, te_acc, win_test_acc)\n",
    "            visualize(vis, epoch+1, valid_acc, win_train_acc)\n",
    "        print(\"epoch=%s: sum_cls_loss=%f, sum_norm_loss=%f, total_loss=%f, train_acc=%f, te_acc=%f\" % (epoch, sum_cls_loss, sum_norm_loss, total_loss, valid_acc, te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- Visdom ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.visdom:\n",
    "    # Visdomを起動\n",
    "    vis = visdom.Visdom(port=args.port)\n",
    "    \n",
    "    # 窓を作成\n",
    "    win_cls_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='cls_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_norm_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='norm_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_train_loss = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='train_loss',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_train_acc = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='train_accuracy',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )\n",
    "    win_test_acc = vis.line(\n",
    "        X=np.array([0]),\n",
    "        Y=np.array([0]),\n",
    "        opts=dict(\n",
    "            title='test_accuracy',\n",
    "            xlabel='epoch',\n",
    "            ylabel='loss',\n",
    "            width=800,\n",
    "            height=400\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- コンフィグの保存 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_logger = Logger(args)\n",
    "args_logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---- メイン処理 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(args.model_root) is False:\n",
    "    os.makedirs(args.model_root)\n",
    "if os.path.exists(args.figure_root) is False:\n",
    "    os.makedirs(args.figure_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_per_valid():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_data(X, Y, train_idx, test_idx, sampling=None):\n",
    "    \"\"\"\n",
    "        交差検証用データ作成\n",
    "        引数:\n",
    "            - X: np.array, 昆虫画像全体\n",
    "            - Y: np.array, ラベル全体\n",
    "            - train_idx: np.array, オーバーサンプリング適用前の学習インデックス\n",
    "            - test_idx: np.array, テストインデックス\n",
    "    \"\"\"\n",
    "    valid_train_idx = adopt_sampling(Y, train_idx, args.sampling)\n",
    "    valid_test_idx = test_idx\n",
    "    xtr, ytr, xte, yte = load_validation_data(X, Y, valid_train_idx, valid_test_idx)\n",
    "    return xtr, ytr, xte, yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, Y, xtr, ytr, xte, yte, bs=20, method_aug=None, size_normalization=None):\n",
    "    \"\"\"\n",
    "        データローダ作成\n",
    "        引数:\n",
    "            - X: np.array, 昆虫画像全体\n",
    "            - Y: np.array, ラベル全体\n",
    "            - xtr: np.array, 学習用昆虫画像\n",
    "            - ytr: np.array, 学習用ラベル\n",
    "            - xte: np.array, テスト用昆虫画像\n",
    "            - yte: np.array, テスト用ラベル\n",
    "            - bs: int, バッチサイズ\n",
    "            - method_aug: [str, ...], 学習時に適用するデータ拡張のリスト\n",
    "            dataset.classification.dataset.create_aug_seqにあるものから選択(複数可)\n",
    "            - size_normalization: str, 昆虫のサイズ分布の正規化方法\n",
    "            [None, \"mu\", \"sigma\", \"mu_sigma\", \"uniform\"]の中から一つ \n",
    "    \"\"\"\n",
    "    train_dataset = insects_dataset(xtr, ytr, training=True, method_aug=method_aug, size_normalization=size_normalization)\n",
    "    train_dataloader = data.DataLoader(train_dataset, bs, num_workers=bs, shuffle=True)\n",
    "    valid_dataset = insects_dataset(xtr, ytr, training=False)\n",
    "    valid_dataloader = data.DataLoader(valid_dataset, 1, num_workers=1, shuffle=False)\n",
    "    test_dataset = insects_dataset(xte, yte, training=False)\n",
    "    test_dataloader = data.DataLoader(test_dataset, 1, num_workers=1, shuffle=False)\n",
    "    all_dataset = insects_dataset(X, Y, training=False)\n",
    "    all_dataloader = data.DataLoader(all_dataset, 1, num_workers=1, shuffle=False)\n",
    "    return train_dataloader, valid_dataloader, test_dataloader, all_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_num = int(1.0/args.test_ratio)\n",
    "with h5py.File(args.all_data_path) as f:\n",
    "    X = f[\"X\"][:]\n",
    "    Y = f[\"Y\"][:]\n",
    "_, ntests = np.unique(Y, return_counts=True)\n",
    "train_idxs, test_idxs = create_validation_split(Y, args.test_ratio)\n",
    "\n",
    "valid_result = []\n",
    "fail_count = np.zeros(Y.shape[0], dtype=\"int\")\n",
    "for valid_count in range(valid_num):\n",
    "    # create validation data\n",
    "    train_idx = train_idxs[valid_count]\n",
    "    test_idx = test_idxs[valid_count]\n",
    "    xtr, ytr, xte, yte = create_validation_data(X, Y, train_idx, test_idx, sampling=args.sampling)\n",
    "    # create dataloader\n",
    "    train_dataloader, valid_dataloader, test_dataloader, all_dataloader = \\\n",
    "    create_dataloader(X, Y, xtr, ytr, xte, yte, bs=args.bs, method_aug=args.method_aug, size_normalization=args.size_normalization)\n",
    "    \n",
    "    # create model (他のモデルもここで宣言すれば使える)\n",
    "    model = ResNet(args.model_name, len(args.labels), pretrain=args.pretrain, param_freeze=args.param_freeze, \\\n",
    "                   use_dropout=args.use_dropout, activation_function=args.activation_function, decoder=args.decoder).cuda()\n",
    "    \n",
    "    # training\n",
    "    train(train_dataloader, valid_dataloader, test_dataloader, model, lr=args.lr, lamda=args.lamda, \\\n",
    "          nepoch=args.nepoch, visdom=args.visdom, label_smoothing=args.label_smoothing, optimizer=args.optimizer)\n",
    "    \n",
    "    # モデルの保存\n",
    "    model.load_state_dict(torch.load(pj(args.model_root, \"valid_\" + str(valid_count + 1) + \"_best.pth\")))\n",
    "    \n",
    "    # 交差検証ごとの評価\n",
    "    model.eval()\n",
    "    matrix = confusion_matrix(test_dataloader, model, args.labels) # テストデータセットの混同行列\n",
    "    valid_result.extend(test_classification(test_dataloader, model)) # テストデータセットの予測の集計\n",
    "    _, correct = accuracy(all_dataloader, model, return_correct=True) # 全データでの実験結果\n",
    "    model.train()\n",
    "    fail_count += ~correct # 予測失敗例を集計\n",
    "    \n",
    "    # 交差検証ごとの混同行列の集計\n",
    "    df = pd.DataFrame(matrix)\n",
    "    display(df)\n",
    "    if valid_count == 0:\n",
    "        validation_matrix = matrix\n",
    "        x_all = xte\n",
    "        y_all = yte\n",
    "    else:\n",
    "        validation_matrix += matrix\n",
    "        x_all = np.concatenate([x_all, xte])\n",
    "        y_all = np.concatenate([y_all, yte])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(validation_matrix)\n",
    "if args.save_df is True:\n",
    "    df.to_csv(pj(args.figure_root, \"validation_matrix.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(validation_matrix, ntests, args.labels, args.figure_root, save=args.save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_df = compute_each_size_df(valid_result, x_all, y_all)\n",
    "if args.save_df is True:\n",
    "    each_df.to_csv(pj(args.figure_root, \"each_size_df.csv\"))\n",
    "each_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = compute_all_size_df(each_df)\n",
    "if args.save_df is True:\n",
    "    all_df.to_csv(pj(args.figure_root, \"all_size_df.csv\"))\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_df = pd.DataFrame({\"fail_count\": fail_count})\n",
    "if args.save_df is True:\n",
    "    fail_df.to_csv(pj(args.figure_root, \"fail_count.csv\"))\n",
    "fail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_distrib_size(all_df, args.figure_root, save=args.save_fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_100",
   "language": "python",
   "name": "pytorch_100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
